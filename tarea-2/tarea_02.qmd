---
title: "Tarea 2: descomposiciones del error de predicción"
format: html
self-contained: true
---

En este ejemplo consideramos la descomposición simplificada al final de la sección 2,
e intentaremos ver para el primer ejemplo de la sección 3 cómo se comporta cada error. Nótese que podemos examinar la descomposición en distintos valores de x:

$$\mathbf{y} - \hat{f_{\mathcal{L}}}(\mathbf{x}) = \underbrace{f^* (\mathbf{x}) - E(\hat{f_{\mathcal{L}}}(\mathbf{x}))}_\text{sesgo} +   \underbrace{E(\hat{f_{\mathcal{L}}}(\mathbf{x})) - \hat{f_{\mathcal{L}}}(\mathbf{x})}_\text{variabilidad} + \underbrace{y - f^*(\mathbf{x})}_\text{irreducible}.$$

En este caso nos reduciremos a dimensión 1 (una variable de entrada). Generamos datos con:

```{r}
#| message: false
library(tidyverse)
library(tidymodels)
fun_exp <- function(x) exp(-16 * sum(x ^ 2))
simular_1 <- function(n, n_dim = 2){
  datos_x <- tibble(x = runif(n_dim * n, -1, 1)) |>
    mutate(nombre = rep(paste("x", 1:n_dim, sep="_"), n)) |> 
    mutate(id = rep(1:n, each = n_dim))
  datos_y <- datos_x |> 
    group_by(id) |> 
    summarise(y = fun_exp(x))
  datos_tbl <- datos_x |> 
    pivot_wider(values_from = x, names_from = nombre) |> 
    left_join(datos_y, by = "id")
  datos_tbl
}
```

 Abajo  mostramos la gráfica que queremos estimar:

```{r}
#| fig-width: 4
#| fig-height: 3
datos_f <- tibble(x_1 = seq(-1, 1, 0.01)) |> 
  mutate(y = map_dbl(x_1, fun_exp)) |> 
  mutate(id = 0)
datos_f |> 
ggplot(aes(x = x_1, y = y)) + geom_line()
```
Ahora construímos un modelo de 1-vecino más cercano, con 
una muestra de $n=10$ casos:

```{r}
set.seed(7251)
muestra <- simular_1(10, 1)
```

```{r}
#modelo
vmc_1 <- nearest_neighbor(neighbors = 1, 
    weight_func = "rectangular") |>
  set_engine("kknn") |>  
  set_mode("regression")
#preprocesamiento
receta_vmc <- recipe(y ~ ., muestra) |> 
  update_role(id, new_role = "id muestra")
#flujo
flujo_vecinos <- workflow() |>  
  add_recipe(receta_vmc) |> 
  add_model(vmc_1)
#ajustar flujo
ajuste_vecinos <- fit(flujo_vecinos, muestra)
predicciones <- predict(ajuste_vecinos, datos_f) |> 
  bind_cols(datos_f) |> 
  pivot_longer(cols = c("y", ".pred"))
```

En la siguiente grpafica observamos la muestra observada, las predicciones
del modelo ajustado, y la curva que representa predicciones perfectas.

```{r}
ggplot(predicciones, aes(x = x_1)) +
  geom_line(aes(y = value, colour = name)) + 
  geom_point(data = muestra, aes(x = x_1, y = y)) +
  scale_colour_manual(values = c("red", "black"))
```

**Pregunta 1**. Argumenta por qué el error irreducible en este ejemplo, para cualquier $x$ es igual a 0 (no hay error irreducible).

El error irreducible está definido como: $y - f^*(\mathbf{x})$, donde $f^*(\mathbf{x})$ se refiere al predictor óptimo posible de generar con la información disponible, el cual es aquella función que minimiza el error de predicción, mientras que $y$ son las observaciones objetivo que queremos obtener con nuestro predictor. En este caso, nuestra variable objetivo es producto de una función exponencial, la cual para cada valor de $x$ asigna un valor único de $y$, y dado que todos nuestros valores de $x$ están sobre un espacio lineal sin repeticiones, entonces nuestro predictor óptimo será aquella función que ajuste a la función exponencial generadora de los valores $y$ dentro del dominio de nuestros valores de $x$. De esta manera, si desarrollamos un predictor óptimo con valores que no tengan varianza, como lo es el caso de este ejercicio, el predictor óptimo siempre arrojará el valor $y$ correcto, provocando que el error irreducible sea 0. 

## Términos de sesgo y variabilidad 

Para calcular los términos de sesgo y varianza es necesario ver
varias muestras y ajustar el modelo. Recuerda que para el término
de sesgo **necesitamos calcular la media de las predicciones sobre
varias muestras de entrenamiento**.

```{r}
reps_tbl <- map_df(1:500, function(rep){
  muestra <- simular_1(10, 1)
  ajuste_vecinos <- fit(flujo_vecinos, muestra)
  predicciones <- predict(ajuste_vecinos, datos_f) |> 
  bind_cols(datos_f) |> select(x_1, .pred)
  predicciones |> mutate(rep = rep)
})
reps_tbl <- reps_tbl |> 
  mutate(y = map_dbl(x_1, fun_exp))
```

Podemos examinar cómo se ve nuestro predictor para distintas muestras:

```{r}
reps_tbl |> pivot_longer(cols= c(".pred","y")) |> 
  filter(rep <= 6) |> 
  ggplot(aes(x=x_1, y = value, group = interaction(rep, name), 
             colour = name)) +
  geom_line() + facet_wrap(~ rep) +
  scale_colour_manual(values = c("red", "black"))
```
Veamos cuáles los valores esperados de las predicciones a lo 
largo de las distintas muestras de entrenamiento:

```{r}
reps_tbl |> 
  group_by(x_1) |> 
  summarise(pred_media = mean(.pred), y = mean(y)) |> 
  pivot_longer(cols = c("pred_media", "y")) |> 
  ggplot(aes(x=x_1, y = value, colour = name)) +
  geom_line() +   scale_colour_manual(values = c("red", "black"))

```
**Pregunta 2**: cuál es el sesgo en x = 0 para este modelo? ¿Por qué pasa eso? ¿Existen otros valores x donde existe sesgo? ¿En qué regiones observas sesgo muy chico?

```{r}
sesgo <- reps_tbl |> 
  group_by(x_1) |> 
  summarise(pred_media = mean(.pred), y = mean(y)) |>
  mutate(sesgo = y - pred_media)

print(paste0("El sesgo en x = 0 es: ", sesgo$sesgo[sesgo$x_1 == 0]))
sesgo[sesgo$x_1 == 0, ]
```

En este caso sufrimos por el **sesgo de estimación**, el cual es causado por no tener un buen tamaño de muestra para que nuestro predictor tienda al predictor óptimo. 

```{r}
sesgo[near(sesgo$sesgo, 0, tol=10^-3), ]
```
Comparando los valores de sesgo cercanos a 0 con una tolerancia de 0.001, vemos que solamente existe un valor con un sesgo muy pequeño que puede interpretarse como aproximadamente 0, por lo que podemos concluir que el resto de los valores tienen al menos un sesgo significativo. Las zonas donde el sesgo es pequeño se pueden observar en la gráfica de nuestro predictor comparado con el predictor óptimo, donde vemos que en las colas antes y después de su punto de inflexión respectivamente hay un sesgo menor, al igual que antes y después del punto de inflexión de la *campana*.

Ahora consideramos la variabilidad. Podemos resumir, por ejemplo,
calculando cuantiles de las predicciones en cada x de interés:

```{r}
reps_tbl |> 
  group_by(x_1) |> 
  summarise(pred_mediana = mean(.pred), y = mean(y), 
            q_90 = quantile(.pred, 0.95), 
            q_10 = quantile(.pred, 0.05)) |> 
  ggplot(aes(x=x_1, y = pred_mediana, ymin = q_10, ymax = q_90)) +
  geom_ribbon(alpha = 0.2) +
  geom_line(colour = "red") +
  geom_line(aes(y = y), colour = "black") 
```
**Pregunta 3**: En qué regiones observas muy poca variablidad? Alrededor de x = 0 aproximadamente qué valores puede tomar el término de variablidad? Alrededor de valores como x=0.25, qué influye más en el error de predicción, el sesgo o la variabilidad?

Se puede observar **poca variabilidad** nuevamente en las colas antes y después de su punto de inflexión respectivamente.

```{r}
variabilidad <- reps_tbl |> 
  group_by(x_1) |> 
  summarise(pred_mediana = mean(.pred), y = mean(y), 
            q_90 = quantile(.pred, 0.95), 
            q_10 = quantile(.pred, 0.05))
x0 = variabilidad[variabilidad$x_1==0, ]
varq10 = x0$q_10 - x0$pred_mediana
varq90 = x0$q_90 - x0$pred_mediana
print(paste0("La variabilidad al rededor de x = 0 puede ser desde ", varq10, " hasta ", varq90))
```
```{r}
sesgo[sesgo$x_1 == 0.25, ]
x25 <-  variabilidad[variabilidad$x_1==0.25, ]
varq10 <- x25$q_10 - x25$pred_mediana
varq90 <- x25$q_90 - x25$pred_mediana
print(paste0("La variabilidad al rededor de x = 0.25 puede ser desde ", varq10, " hasta ", varq90))
```

Con los valores anteriores podemos notar que la variabilidad es muy alta para x = 0.25, mientras que el sesgo es menor a 0.1, por lo que podemos concluir que influye más la variabilidad en el error de predicción.

**Pregunta 4** En la práctica, sólo tenemos una muestra para hacer nuestras predicciones. Explica en tus palabras por qué nos preocupa que la variabilidad sea grande.
Nos preocupa porque buscamos generalizar el comportamiento de una población mediante una muestra desarrollando un predictor que se asemeje lo más posible a la distribución de probabilidad original de la muestra, por lo que si tenemos mucha variabilidad en nuestro modelo implica que podemos tener valores poco realistas o cercanos a los valores originales, lo que involucra incertudumbre muy alta y posiblemente sea un modelo que no represente la distribución original 


## Tipo de sesgo

**Pregunta 5**: discute y experimenta lo que sucede si tomas una muestra más grande (por ejemplo n=30) en el ejemplo anterior. ¿Qué pasa con el sesgo y la variabilidad?

```{r}
reps_tbl <- map_df(1:500, function(rep){
  muestra <- simular_1(30, 1)
  ajuste_vecinos <- fit(flujo_vecinos, muestra)
  predicciones <- predict(ajuste_vecinos, datos_f) |> 
  bind_cols(datos_f) |> select(x_1, .pred)
  predicciones |> mutate(rep = rep)
})
reps_tbl <- reps_tbl |> 
  mutate(y = map_dbl(x_1, fun_exp))
```

```{r}
reps_tbl |> pivot_longer(cols= c(".pred","y")) |> 
  filter(rep <= 6) |> 
  ggplot(aes(x=x_1, y = value, group = interaction(rep, name), 
             colour = name)) +
  geom_line() + facet_wrap(~ rep) +
  scale_colour_manual(values = c("red", "black"))
```

```{r}
reps_tbl |> 
  group_by(x_1) |> 
  summarise(pred_media = mean(.pred), y = mean(y)) |> 
  pivot_longer(cols = c("pred_media", "y")) |> 
  ggplot(aes(x=x_1, y = value, colour = name)) +
  geom_line() +   scale_colour_manual(values = c("red", "black"))

```

```{r}
reps_tbl |> 
  group_by(x_1) |> 
  summarise(pred_mediana = mean(.pred), y = mean(y), 
            q_90 = quantile(.pred, 0.95), 
            q_10 = quantile(.pred, 0.05)) |> 
  ggplot(aes(x=x_1, y = pred_mediana, ymin = q_10, ymax = q_90)) +
  geom_ribbon(alpha = 0.2) +
  geom_line(colour = "red") +
  geom_line(aes(y = y), colour = "black") 
```
De las gráficas anteriores podemos observar que el predictor generado se asimila mucho al predictor óptimo, por lo que el sesgo se reduce considerablemente en comparación con el modelo desarrollado con muestras de tamaño 10. Adicionalmente, la variabilidad también se reduce considerablemente, teniendo una región de mayor variabilidad al rededor de x=0.

**Pregunta 6**: Descomposición del sesgo: explica por qué este modelo no tiene sesgo de especificación (cuando tomamos una muestra muy grande), y que su problema principal es el sesgo de estimación. ¿Qué pasaría si ajustáramos en lugar de 1 vecino más cercano un modelo de regresión? ¿Qué tipo de sesgo sería más grande?

Dado que tratamos de generalizar una función continua donde para cada valor de $x$ existe un valor único de $y$, al incrementar el tamaño de muestra estamos representando cada vez con más finura la función objetivo, lo que le permite al predictor tender a ser el predictor óptimo y minimizar el **sesgo de especificación**. Por otro lado, el **sesgo de estimación** se ve influenciado por la naturaleza de la muestra y su tamaño, por lo que generar un único predictor usando este método de KNN con una muestra específica no nos daría mucha certeza de que los valores obtenidos a la salida sean los correctos, lo que nos requeriría desarrollar más de un modelo con distintas muestras y obtener un promedio de sus salidas para brindar más confianza sobre el valor que deseamos estimar. 

La regresión es un modelo que sufre más de sesgo de especificación ya que, aunque se le proporcionen todos los datos de la población, el modelo se alejaría del predictor óptimo ya que no alcanzaría a capturar las corvaturas de la función objetivo. En cambio, el sesgo de estimación sería muy pequeño ya que la recta generada variaría su intercepto por una cantidad muy pequeña, y la pendiente tendería a ser la misma, lo que haría que el sesgo de estimación fuera también diminuto. 

**Pregunta 7**: revisa el ejemplo que vimos en clase de la maldición de la dimensionalidad. 

```{r}
fun_exp <- function(x) exp(-8 * sum(x ^ 2))
x <- map(1:1000, ~ runif(2, -1, 1))
dat <- tibble(x = x) |> 
        mutate(y = map_dbl(x, fun_exp))
```

```{r}
ggplot(dat |> mutate(x_1 = map_dbl(x, 1), x_2 = map_dbl(x, 2)), 
       aes(x = x_1, y = x_2, colour = y)) + geom_point()
```
```{r}
dat <- dat |> mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) |> 
  arrange(dist_origen)
mas_cercano <- dat[1, ]
mas_cercano
```

```{r}
mas_cercano$x[[1]]
```
Ahora para dimensión p=8

```{r}
x <- map(1:1000000, ~ runif(8, -1, 1))
dat <- tibble(x = x) |> 
       mutate(y = map_dbl(x, fun_exp))
dat <- dat |> mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) |> 
  arrange(dist_origen)
mas_cercano <- dat[1, ]
mas_cercano
```

```{r}
mas_cercano$x[[1]]
```
```{r}
mas_cercano$y
```
¿Qué es lo que está pasando? La razón es que en dimensiones altas, los puntos de la muestra de entrenamiento están muy lejos unos de otros, y están cerca de la frontera, incluso para tamaños de muestra relativamente grandes como n = 1000. Cuando la dimensión crece, la situación empeora exponencialmente.

#### ¿Por qué el sesgo de estimación es tan grande en ese caso (con muestra de mil casos y dimensión 8? Explica por qué no hay mucha variabilidad en ese ejemplo particular, pero las predicciones en x=0 son muy malas. ¿El problema es sesgo de estimación o de especificación?

El sesgo de estimación es grande porque necesitamos aumentar considerablemente la muestra para poder llenar los espacios entre vecinos y evitar que un nuevo punto tome valores que no le corresponden. No hay mucha varaibilidad porque estamos trabajando sobre un espacio entre -1 y 1, y la mayoría de los puntos están concentrados para valores entre 0.0 y 0.5, lo que hace que las predicciones sean considerablemente consistentes en ese rango de y. 

Es un problema de sesgo de estimación, ya que conforme aumenta el tamaño de la muestra, se acerca cada vez más al valor del estimador óptimo, por lo que en caso de ser sesgo de especificación entonces nuestro modelo no se desempeñaría mejor aunque aumentemos el tamaño de muestra a infinito. 

## Resumen varianza sesgo (opcional)

Si usamos el error cuadrático medio, puede demostrarse que de la ecuación de arriba se puede obtener:

$$Err(x) = \textrm{Sesgo}(x)^2 + \textrm{Var}(\hat{f}_{\mathcal L} (x)) + \sigma^ 2(x)$$
donde $\textrm{Sesgo}(x) = f^*(x) - E(\hat{f}_{\mathcal L}(x))$, y $\sigma^2(x)$ es la varianza del error irreducible en $x$.

Podemos calcular para nuestro ejemplo

```{r}
resumen_tbl <- reps_tbl |> 
  group_by(x_1) |> 
  summarise(pred_media = mean(.pred), y = mean(y),
            error_cuad = mean((y - .pred)^2),
            sesgo2 = (pred_media - y)^2,
            varianza = var(.pred - pred_media),
            sigma2 = 0^2)
resumen_tbl |> filter(x_1 == 0) |> round(3)
```
```{r}
resumen_tbl |> filter(x_1 == 0.25) |> round(3)
```
**Pregunta 7** (opcional) Verifica que en todos los casos el error cuadrático es igual al sesgo2 más la varianza más sigma2.

```{r}
resumen_tbl <- resumen_tbl |> mutate(error_cuad_hat = sesgo2 + varianza)
resumen_tbl
```

```{r}
resumen_tbl |> 
  mutate(diff_error = abs(error_cuad_hat - error_cuad)) |> 
  select(x_1, error_cuad, error_cuad_hat, diff_error) |> arrange(desc(diff_error))
```

